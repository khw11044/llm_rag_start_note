{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT4All\n",
    "\n",
    "- https://wikidocs.net/233806\n",
    "\n",
    "https://github.com/nomic-ai/gpt4all\n",
    "\n",
    "GitHub:nomic-ai/gpt4all은 코드, 스토리, 대화를 포함한 방대한 양의 깨끗한 어시스턴트 데이터로 학습된 오픈 소스 챗봇 생태계입니다.\n",
    "\n",
    "이 예제에서는 LangChain을 사용하여 GPT4All 모델과 상호 작용하는 방법에 대해 설명합니다.\n",
    "\n",
    "공식 홈페이지 \n",
    "- https://gpt4all.io/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gpt4all\n",
      "  Downloading gpt4all-2.7.0-py3-none-win_amd64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\kim_h\\anaconda3\\envs\\rag\\lib\\site-packages (from gpt4all) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kim_h\\anaconda3\\envs\\rag\\lib\\site-packages (from gpt4all) (4.66.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kim_h\\anaconda3\\envs\\rag\\lib\\site-packages (from requests->gpt4all) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kim_h\\anaconda3\\envs\\rag\\lib\\site-packages (from requests->gpt4all) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kim_h\\anaconda3\\envs\\rag\\lib\\site-packages (from requests->gpt4all) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kim_h\\anaconda3\\envs\\rag\\lib\\site-packages (from requests->gpt4all) (2024.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\kim_h\\anaconda3\\envs\\rag\\lib\\site-packages (from tqdm->gpt4all) (0.4.6)\n",
      "Downloading gpt4all-2.7.0-py3-none-win_amd64.whl (28.6 MB)\n",
      "   ---------------------------------------- 0.0/28.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/28.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/28.6 MB 435.7 kB/s eta 0:01:06\n",
      "    --------------------------------------- 0.6/28.6 MB 5.8 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.7/28.6 MB 10.9 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.8/28.6 MB 18.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 6.4/28.6 MB 25.7 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 9.1/28.6 MB 30.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 9.6/28.6 MB 30.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 9.7/28.6 MB 26.9 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 9.8/28.6 MB 22.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 9.9/28.6 MB 21.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 10.2/28.6 MB 19.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 10.6/28.6 MB 22.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 10.6/28.6 MB 22.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 10.7/28.6 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 10.8/28.6 MB 17.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 10.9/28.6 MB 16.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 11.0/28.6 MB 14.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 11.3/28.6 MB 13.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 11.7/28.6 MB 13.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 11.7/28.6 MB 13.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 11.8/28.6 MB 12.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 11.8/28.6 MB 11.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 12.0/28.6 MB 11.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 12.1/28.6 MB 10.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 12.3/28.6 MB 10.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 13.9/28.6 MB 10.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 16.5/28.6 MB 9.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 19.0/28.6 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 21.7/28.6 MB 21.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 24.3/28.6 MB 59.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 26.9/28.6 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  28.6/28.6 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.6/28.6 MB 46.7 MB/s eta 0:00:00\n",
      "Installing collected packages: gpt4all\n",
      "Successfully installed gpt4all-2.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "������ ��θ� ã�� �� �����ϴ�.\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU gpt4all > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 다운로드\n",
    "\n",
    "\n",
    "gpt4all 페이지에는 Model Explorer 섹션이 있습니다. (더 많은 정보를 원하시면 https://github.com/nomic-ai/gpt4all 을 방문하세요.)\n",
    "\n",
    "공식 홈페이지 에서 다운로드 가능한 모델을 다운로드 받습니다. 본인의 PC 사양에서 구동가능한 모델을 선택하는 것이 좋습니다.\n",
    "\n",
    "본 튜토리얼에서는 nous-hermes-llama2-13b.Q4_0.gguf(7.37GB) 모델을 다운로드 받아 진행하겠습니다.\n",
    "\n",
    "다운로드 받은 모델은 **models 폴더** 생성 후 해당 폴더에 다운로드 받습니다.\n",
    "\n",
    "local_path 변수에 로컬 파일 경로(\"./models/ggml-gpt4all-l13b-snoozy.bin\")를 할당합니다.\n",
    "\n",
    "이 경로는 사용자가 원하는 로컬 파일 경로로 대체할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_file = '../models/mistral-7b-openorca.gguf2.Q4_0.gguf'\n",
    "\n",
    "local_path = (\n",
    "    model_file  # 원하는 로컬 파일 경로로 대체하세요.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT4ALL 모델 활용\n",
    "\n",
    "GPT4All은 GPT-3와 유사한 대규모 언어 모델로, 다양한 자연어 처리 작업에 활용될 수 있습니다.\n",
    "\n",
    "이 모듈을 사용하면 GPT4All 모델을 간편하게 로드하고 추론에 활용할 수 있습니다.\n",
    "\n",
    "- GPT4All 언어 모델을 사용하여 프롬프트에 대한 응답을 생성하는 LLMChain을 구현합니다.\n",
    "- PromptTemplate을 사용하여 프롬프트 템플릿을 정의합니다.\n",
    "- StreamingStdOutCallbackHandler를 사용하여 언어 모델의 출력을 실시간으로 스트리밍합니다.\n",
    "- 사용자 정의 모델을 사용하려면 backend 매개변수를 추가합니다. 지원되는 백엔드는 GPT4All Python 문서에서 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"Name any five companies which makes `{product}`?\",\n",
    ")\n",
    "\n",
    "# GPT4All 언어 모델 초기화\n",
    "# model는 GPT4All 모델 파일의 경로를 지정\n",
    "llm = GPT4All(\n",
    "    model=local_path,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    # backend=\"gpu\", # GPU 설정\n",
    "    streaming=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 체인 생성\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 질의 실행\n",
    "response = chain.invoke({\"product\": \"Smart phone\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
