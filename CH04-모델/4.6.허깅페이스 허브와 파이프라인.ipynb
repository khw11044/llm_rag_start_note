{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 정보로드를 위한 라이브러리\n",
    "# 설치: pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 토큰 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스 모델/토크나이저를 다운로드 받을 경로\n",
    "# (예시)\n",
    "\n",
    "import os\n",
    "\n",
    "# ./cache/ 경로에 다운로드 받도록 설정\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"./cache/\"\n",
    "os.environ[\"HF_HOME\"] = \"./cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "import warnings\n",
    "\n",
    "# 경고 메시지 무시\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# HuggingFace Repository ID\n",
    "# \"KT-AI/midm-bitext-S-7B-inst-v1\"\n",
    "# \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# HuggingFaceHub 객체 생성\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id,\n",
    "    model_kwargs={\"temperature\": 0.1, \"max_length\": 2048},\n",
    "    task=\"text-generation\",  # 텍스트 생성\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"Summarizes TEXT in simple bullet points ordered from most important to least important.\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "KeyPoints: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "text = \"\"\"A Large Language Model (LLM) like me, ChatGPT, is a type of artificial intelligence (AI) model designed to understand, generate, and interact with human language. These models are \"large\" because they're built from vast amounts of text data and have billions or even trillions of parameters. Parameters are the aspects of the model that are learned from training data; they are essentially the internal settings that determine how the model interprets and generates language. LLMs work by predicting the next word in a sequence given the words that precede it, which allows them to generate coherent and contextually relevant text based on a given prompt. This capability can be applied in a variety of ways, from answering questions and composing emails to writing essays and even creating computer code. The training process for these models involves exposing them to a diverse array of text sources, such as books, articles, and websites, allowing them to learn language patterns, grammar, facts about the world, and even styles of writing. However, it's important to note that while LLMs can provide information that seems knowledgeable, their responses are generated based on patterns in the data they were trained on and not from a sentient understanding or awareness. The development and deployment of LLMs raise important considerations regarding accuracy, bias, ethical use, and the potential impact on various aspects of society, including employment, privacy, and misinformation. Researchers and developers continue to work on ways to address these challenges while improving the models' capabilities and applications.\"\"\"\n",
    "\n",
    "response = llm_chain.invoke(input=text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizes TEXT in simple bullet points ordered from most important to least important.\n",
      "TEXT:\n",
      "A Large Language Model (LLM) like me, ChatGPT, is a type of artificial intelligence (AI) model designed to understand, generate, and interact with human language. These models are \"large\" because they're built from vast amounts of text data and have billions or even trillions of parameters. Parameters are the aspects of the model that are learned from training data; they are essentially the internal settings that determine how the model interprets and generates language. LLMs work by predicting the next word in a sequence given the words that precede it, which allows them to generate coherent and contextually relevant text based on a given prompt. This capability can be applied in a variety of ways, from answering questions and composing emails to writing essays and even creating computer code. The training process for these models involves exposing them to a diverse array of text sources, such as books, articles, and websites, allowing them to learn language patterns, grammar, facts about the world, and even styles of writing. However, it's important to note that while LLMs can provide information that seems knowledgeable, their responses are generated based on patterns in the data they were trained on and not from a sentient understanding or awareness. The development and deployment of LLMs raise important considerations regarding accuracy, bias, ethical use, and the potential impact on various aspects of society, including employment, privacy, and misinformation. Researchers and developers continue to work on ways to address these challenges while improving the models' capabilities and applications.\n",
      "\n",
      "KeyPoints: \n",
      "- ChatGPT is a Large Language Model (LLM) that understands, generates, and interacts with human language.\n",
      "- LLMs are \"large\" because they're built from vast amounts of text data and have billions or even trillions of parameters.\n",
      "- Parameters are the aspects of the model that are learned from training data; they determine how the model interprets and generates language.\n",
      "- LLMs work by predicting the next word in a sequence given\n"
     ]
    }
   ],
   "source": [
    "print(response[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 허깅페이스 파이프라인(HuggingFace Pipeline)\n",
    "\n",
    "__Hugging Face Local Pipelines__\n",
    "\n",
    "뭔가 세부 옵션 튜닝 느낌?\n",
    "\n",
    "HuggingFacePipeline 클래스를 통해 Hugging Face 모델을 로컬에서 실행할 수 있습니다.\n",
    "\n",
    "Hugging Face Model Hub는 온라인 플랫폼에서 120,000개 이상의 모델, 20,000개의 데이터셋, 50,000개의 데모 앱(Spaces)을 호스팅하며, 모두 오픈 소스이고 공개적으로 사용 가능\n",
    "\n",
    "이러한 모델은 LangChain에서 이 로컬 파이프라인 래퍼를 통해 호출하거나, HuggingFaceHub 클래스를 통해 호스팅된 추론 엔드포인트를 호출하여 사용할 수 있습니다.\n",
    "\n",
    "사용하기 위해서는 PyTorch와 함께 Python 패키지 transformers가 설치되어 있어야 합니다.\n",
    "\n",
    "또한, 보다 메모리 효율적인 attention 구현을 위해 xformer를 설치할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet  transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스 모델/토크나이저를 다운로드 받을 경로\n",
    "# (예시)\n",
    "import os\n",
    "\n",
    "# ./cache/ 경로에 다운로드 받도록 설정\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"./cache/\"\n",
    "os.environ[\"HF_HOME\"] = \"./cache/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading\n",
    "\n",
    "모델은 from_model_id 메서드를 사용하여 모델 매개변수를 지정함으로써 로드할 수 있습니다.\n",
    "\n",
    "- HuggingFacePipeline 클래스를 사용하여 Hugging Face의 사전 학습된 모델을 로드합니다.\n",
    "- from_model_id 메서드를 사용하여 beomi/llama-2-ko-7b 모델을 지정하고, 작업을 \"text-generation\"으로 설정합니다.\n",
    "- pipeline_kwargs 매개변수를 사용하여 생성할 최대 토큰 수를 10으로 제한합니다.\n",
    "- 로드된 모델은 hf 변수에 할당되며, 이를 통해 텍스트 생성 작업을 수행할 수 있습니다.\n",
    "- 사용한 모델: https://huggingface.co/beomi/llama-2-ko-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40300c78f1647d09e358c8d26d9a361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20930962cf33492687e86d22d26fbcbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00015.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea6b983fecc418abffff8652aa20f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00015.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a65106e6d147a99b61c7947dcc7a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00015.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9f9ce7db744851a97d53d77d0b4c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00015.safetensors:   0%|          | 0.00/944M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dadf353f2d9d4a72878e3f527fe53b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00015.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a2a69178c1403fb4b2104da71cdda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00015.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ecc984cef42436da521908964d8e986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00015.safetensors:   0%|          | 0.00/742M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722f9d79b12642fa9ac14d94f297bbb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00015-of-00015.safetensors:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30178176e3df49fda7092e491e53f96d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6310df9329014d619e43f55f4e0c2515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# HuggingFace 모델을 다운로드 받습니다.\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"beomi/llama-2-ko-7b\",  # 사용할 모델의 ID를 지정합니다.\n",
    "    task=\"text-generation\",  # 수행할 작업을 지정합니다. 여기서는 텍스트 생성입니다.\n",
    "    # 파이프라인에 전달할 추가 인자를 설정합니다. 여기서는 생성할 최대 토큰 수를 10으로 제한합니다.\n",
    "    pipeline_kwargs={\"max_new_tokens\": 512},\n",
    ")\n",
    "\n",
    "# 기존의 transformers pipeline을 직접 전달하여 로드할 수도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFacePipeline을 사용하여 텍스트 생성 모델을 구현합니다.\n",
    "\n",
    "- AutoTokenizer와 AutoModelForCausalLM을 사용하여, \"beomi/llama-2-ko-7b 모델\"과 \"토크나이저\"를 로드합니다.\n",
    "- pipeline 함수를 사용하여 \"text-generation\" 파이프라인을 생성하고, \"모델과 토크나이저\"를 설정합니다. 최대 생성 토큰 수는 10으로 제한합니다.\n",
    "- HuggingFacePipeline 클래스를 사용하여 hf 객체를 생성하고, 생성된 파이프라인을 전달합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 생성된 hf 객체를 사용하여 주어진 프롬프트에 대한 텍스트 생성을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d5f1317e244107be6850c56deb6088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d86947264824358a5d1972c0b48d633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"beomi/llama-2-ko-7b\"  # 사용할 모델의 ID를 지정합니다.\n",
    "\n",
    "# 지정된 모델의 토크나이저를 로드합니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)  \n",
    "\n",
    " # 지정된 모델을 로드합니다.\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id) \n",
    "\n",
    "# 텍스트 생성 파이프라인을 생성하고, 최대 생성할 새로운 토큰 수를 10으로 설정합니다.\n",
    "pipe = pipeline(\"text-generation\", \n",
    "                model=model,\n",
    "                tokenizer=tokenizer, \n",
    "                max_new_tokens=512)\n",
    "\n",
    "# HuggingFacePipeline 객체를 생성하고, 생성된 파이프라인을 전달합니다.\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Chain\n",
    "\n",
    "모델이 메모리에 로드되면 프롬프트와 함께 구성하여 체인을 형성할 수 있습니다.\n",
    "\n",
    "- PromptTemplate 클래스를 사용하여 질문과 답변 형식을 정의하는 프롬프트 템플릿을 생성합니다.\n",
    "- prompt 객체와 hf 객체를 파이프라인으로 연결하여 chain 객체를 생성합니다.\n",
    "- chain.invoke() 메서드를 호출하여 주어진 질문에 대한 답변을 생성하고 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 답변은 한국어로 하기 \n",
    "template = \"\"\"Answer the following question in Korean.\n",
    "#Question: \n",
    "{question}\n",
    "\n",
    "#Answer: \"\"\"  # 질문과 답변 형식을 정의하는 템플릿\n",
    "prompt = PromptTemplate.from_template(template)  # 템플릿을 사용하여 프롬프트 객체 생성\n",
    "\n",
    "# 프롬프트와 언어 모델을 연결하여 체인 생성\n",
    "chain = prompt | hf | StrOutputParser()\n",
    "\n",
    "question = \"대한민국의 수도는 어디야?\"  # 질문 정의\n",
    "\n",
    "print(\n",
    "    chain.invoke({\"question\": question})\n",
    ")  # 체인을 호출하여 질문에 대한 답변 생성 및 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Inference\n",
    "\n",
    "GPU에서 실행할 때는 device=n 매개변수를 지정하여 모델을 특정 디바이스에 배치할 수 있습니다.\n",
    "\n",
    "기본값은 -1로, CPU에서 추론을 수행합니다.\n",
    "\n",
    "여러 개의 GPU가 있거나 모델이 단일 GPU에 비해 너무 큰 경우에는 device_map=\"auto\"를 지정할 수 있습니다.\n",
    "\n",
    "이 경우 Accelerate 라이브러리가 필요하며, 모델 가중치를 어떻게 로드할지 자동으로 결정하는 데 사용됩니다.\n",
    "\n",
    "주의: device와 device_map은 함께 지정해서는 안 되며, 예기치 않은 동작을 유발할 수 있습니다.\n",
    "\n",
    "- HuggingFacePipeline을 사용하여 gpt2 모델을 로드하고, device 매개변수를 0으로 설정하여 GPU에서 실행되도록 합니다.\n",
    "- pipeline_kwargs 매개변수를 사용하여 생성할 최대 토큰 수를 10으로 제한합니다.\n",
    "- prompt와 gpu_llm을 파이프라인으로 연결하여 gpu_chain을 생성합니다.\n",
    "- gpu_chain.invoke() 메서드를 호출하여 주어진 질문에 대한 답변을 생성하고 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"beomi/llama-2-ko-7b\",  # 사용할 모델의 ID를 지정합니다.\n",
    "    task=\"text-generation\",  # 수행할 작업을 설정합니다. 여기서는 텍스트 생성입니다.\n",
    "    device=0,       # 사용할 GPU 디바이스 번호를 지정합니다. \"auto\"로 설정하면 accelerate 라이브러리를 사용합니다.\n",
    "    pipeline_kwargs={\"max_new_tokens\": 64},   # 파이프라인에 전달할 추가 인자를 설정합니다. 여기서는 생성할 최대 토큰 수를 10으로 제한합니다.\n",
    ")\n",
    "\n",
    "gpu_chain = prompt | gpu_llm  # prompt와 gpu_llm을 연결하여 gpu_chain을 생성합니다.\n",
    "\n",
    "# 프롬프트와 언어 모델을 연결하여 체인 생성\n",
    "gpu_chain = prompt | gpu_llm | StrOutputParser()\n",
    "\n",
    "question = \"대한민국의 수도는 어디야?\"  # 질문 정의\n",
    "\n",
    "# 체인을 호출하여 질문에 대한 답변 생성 및 출력\n",
    "print(gpu_chain.invoke({\"question\": question}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch GPU Inference\n",
    "\n",
    "GPU 장치에서 실행하는 경우, 배치 모드로 GPU에서 추론을 실행할 수 있습니다.\n",
    "\n",
    "- HuggingFacePipeline을 사용하여 beomi/llama-2-ko-7b 모델을 로드하고, GPU에서 실행되도록 설정합니다.\n",
    "- gpu_llm을 생성할 때 batch_size를 2로 설정하고, temperature를 0으로, max_length를 64로 설정합니다.\n",
    "- prompt와 gpu_llm을 파이프라인으로 연결하여 gpu_chain을 생성하고, 종료 토큰을 \"\\n\\n\"로 설정합니다.\n",
    "- gpu_chain.batch()를 사용하여 questions의 질문들에 대한 답변을 병렬로 생성합니다.\n",
    "- 생성된 답변을 반복문을 통해 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"beomi/llama-2-ko-7b\",  # 사용할 모델의 ID를 지정합니다.\n",
    "    task=\"text-generation\",  # 수행할 작업을 설정합니다.\n",
    "    device=0,  # GPU 디바이스 번호를 지정합니다. -1은 CPU를 의미합니다.\n",
    "    batch_size=2,  # 배치 크기s를 조정합니다. GPU 메모리와 모델 크기에 따라 적절히 설정합니다.\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"max_length\": 256,\n",
    "    },  # 모델에 전달할 추가 인자를 설정합니다.\n",
    ")\n",
    "\n",
    "# 프롬프트와 언어 모델을 연결하여 체인을 생성합니다.\n",
    "gpu_chain = prompt | gpu_llm.bind(stop=[\"\\n\\n\"])\n",
    "\n",
    "questions = []\n",
    "for i in range(4):\n",
    "    # 질문 리스트를 생성합니다.\n",
    "    questions.append({\"question\": f\"숫자 {i} 이 한글로 뭐에요?\"})\n",
    "\n",
    "answers = gpu_chain.batch(questions)  # 질문 리스트를 배치 처리하여 답변을 생성합니다.\n",
    "for answer in answers:\n",
    "    print(answer)  # 생성된 답변을 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
